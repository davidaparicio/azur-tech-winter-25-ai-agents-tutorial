{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prendre en main LangChain pas √† pas\n",
    "\n",
    "Objectif de ce notebook :  \n",
    "- Comprendre les **briques de base** de LangChain (LLM, prompt, chain).  \n",
    "- Visualiser un **sch√©ma simple** de pipeline.  \n",
    "- Construire un mini **pipeline de question/r√©ponse**, puis un mini **pipeline RAG simplifi√©**.\n",
    "\n",
    "> Remarque : ce notebook suppose que vous avez une cl√© API (`OPENAI_API_KEY`) d√©finie dans vos variables d‚Äôenvironnement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les briques de base de LangChain\n",
    "\n",
    "On va manipuler 3 briques principales :\n",
    "\n",
    "1. **LLM** : le mod√®le de langage (par ex. `gpt-4o-mini`)  \n",
    "2. **Prompt** : la mani√®re dont on parle au mod√®le (system + user + variables)  \n",
    "3. **Chain** : l‚Äôassemblage **d√©claratif** des √©tapes (pipeline de traitement)\n",
    "\n",
    "On peut imaginer ce sch√©ma tr√®s simple :\n",
    "\n",
    "```text\n",
    "              +---------------------------+\n",
    "User input -> |   PromptTemplate          | -> texte format√©\n",
    "              +---------------------------+\n",
    "                           |\n",
    "                           v\n",
    "                     +-----------+\n",
    "                     |   LLM     | -> r√©ponse brute du mod√®le\n",
    "                     +-----------+\n",
    "                           |\n",
    "                           v\n",
    "                   +------------------+\n",
    "                   | Output Parser    | -> string / JSON / etc.\n",
    "                   +------------------+\n",
    "```  \n",
    "LangChain permet de **d√©crire ce pipeline en Python**, plut√¥t que de tout reprogrammer √† la main."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Installation (√† faire une fois dans votre environnement)\n",
    "\n",
    "D√©commentez et ex√©cutez la cellule ci-dessous si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-core langchain-openai\n",
    "# Optionnel : outils suppl√©mentaires\n",
    "# !pip install -U langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Initialisation du LLM\n",
    "\n",
    "On cr√©e un objet `ChatOpenAI` qui est la brique \"mod√®le de langage\" c√¥t√© LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10c58b9a0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10c58bf10>, root_client=<openai.OpenAI object at 0x10c589600>, root_async_client=<openai.AsyncOpenAI object at 0x10c58ab00>, model_name='gpt-4o-mini', temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from getpass import getpass\n",
    "\n",
    "# IMPORTANT: Configurez votre cl√© API OpenAI dans vos variables d'environnement\n",
    "# Vous pouvez les d√©finir via un fichier .env ou en les exportant dans votre shell:\n",
    "# export OPENAI_API_KEY=\"your-key-here\"\n",
    "# Ou utilisez getpass pour la saisir de mani√®re s√©curis√©e:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass(\"Entrez votre cl√© API OpenAI: \")\n",
    "\n",
    "# V√©rification que la cl√© est d√©finie\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    raise ValueError(\"OPENAI_API_KEY doit √™tre d√©finie dans vos variables d'environnement. \"\n",
    "                     \"Voir le README.md pour les instructions de configuration.\")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  \n",
    "    temperature=0.2,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Premier appel au LLM via LangChain\n",
    "\n",
    "On commence par l‚Äôusage le plus simple : on envoie une liste de messages au mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un mod√®le de langage est un syst√®me informatique con√ßu pour comprendre et g√©n√©rer du texte en se basant sur des donn√©es linguistiques. Il utilise des algorithmes d'apprentissage automatique pour pr√©dire la probabilit√© des mots ou des phrases dans un contexte donn√©.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Tu es un assistant p√©dagogique, clair et concis.\"),\n",
    "    HumanMessage(content=\"Explique en deux phrases ce qu'est un mod√®le de langage.\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ S√©parer le **prompt** et le **mod√®le**\n",
    "\n",
    "Plut√¥t que de construire les messages √† la main, on d√©crit un **template de conversation**.\n",
    "\n",
    "On utilise `ChatPromptTemplate` pour d√©finir :\n",
    "- le r√¥le syst√®me (persona),\n",
    "- le message utilisateur avec une **variable `{question}`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Un LLM (Large Language Model) et LangChain sont deux concepts diff√©rents, mais ils peuvent travailler ensemble dans le domaine du traitement du langage naturel.\\n\\n### LLM (Large Language Model)\\nUn LLM est un mod√®le d'intelligence artificielle entra√Æn√© sur de vastes ensembles de donn√©es textuelles. Ces mod√®les, comme GPT-3 ou GPT-4, sont capables de comprendre et de g√©n√©rer du texte de mani√®re coh√©rente. Ils peuvent √™tre utilis√©s pour diverses t√¢ches, telles que :\\n\\n- La g√©n√©ration de texte\\n- La traduction\\n- La r√©ponse √† des questions\\n- La r√©daction de contenu\\n\\nLes LLMs fonctionnent en pr√©disant le mot suivant dans une phrase, en se basant sur le contexte fourni par les mots pr√©c√©dents.\\n\\n### LangChain\\nLangChain, en revanche, est une biblioth√®que con√ßue pour faciliter l'int√©gration et l'utilisation des LLMs dans des applications plus complexes. Elle permet de cr√©er des cha√Ænes de traitement de texte qui combinent plusieurs √©tapes, comme :\\n\\n- L'interaction avec des LLMs\\n- La gestion de la m√©moire (pour garder le contexte entre les requ√™tes)\\n- L'int√©gration avec des API externes ou des bases de donn√©es\\n- La cr√©ation de flux de travail personnalis√©s\\n\\nLangChain permet donc de construire des applications qui exploitent la puissance des LLMs tout en ajoutant des fonctionnalit√©s suppl√©mentaires et en g√©rant la logique d'application.\\n\\n### En r√©sum√©\\n- **LLM** : Un mod√®le de langage puissant capable de g√©n√©rer et de comprendre du texte.\\n- **LangChain** : Une biblioth√®que qui facilite l'utilisation des LLMs dans des applications plus complexes, en ajoutant des fonctionnalit√©s et en orchestrant des interactions.\\n\\nEn gros, vous pouvez voir LangChain comme un outil qui vous aide √† tirer le meilleur parti des LLMs dans vos projets de d√©veloppement.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 377, 'prompt_tokens': 42, 'total_tokens': 419, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_50906f2aac', 'id': 'chatcmpl-CiIQRQEQOHxeUzzMBixMm6jYC3vLf', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e73d4673-3c25-46dc-8da9-65213ff673a4-0' usage_metadata={'input_tokens': 42, 'output_tokens': 377, 'total_tokens': 419, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", f\"Tu es un assistant IA qui vulgarise pour un public de d√©veloppeurs curieux.\"),\n",
    "        (\"human\", \"Question : {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# LCEL: LangChain Expression Language\n",
    "chain = prompt | model \n",
    "\n",
    "answer = chain.invoke({\"question\": \"Quelle est la diff√©rence entre un LLM et LangChain ?\"})\n",
    "print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a14162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Un LLM (Large Language Model) et LangChain sont deux concepts diff√©rents, mais ils peuvent travailler ensemble dans le domaine du traitement du langage naturel.\\n\\n### LLM (Large Language Model)\\nUn LLM est un mod√®le d'intelligence artificielle entra√Æn√© sur de vastes ensembles de donn√©es textuelles. Ces mod√®les, comme GPT-3 ou GPT-4, sont capables de comprendre et de g√©n√©rer du texte de mani√®re coh√©rente. Ils peuvent √™tre utilis√©s pour diverses t√¢ches, telles que :\\n\\n- La g√©n√©ration de texte\\n- La traduction\\n- La r√©ponse √† des questions\\n- La r√©daction de contenu\\n\\nLes LLMs fonctionnent en pr√©disant le mot suivant dans une phrase, en se basant sur le contexte fourni par les mots pr√©c√©dents.\\n\\n### LangChain\\nLangChain, en revanche, est une biblioth√®que con√ßue pour faciliter l'int√©gration et l'utilisation des LLMs dans des applications plus complexes. Elle permet de cr√©er des cha√Ænes de traitement de texte qui combinent plusieurs √©tapes, comme :\\n\\n- L'interaction avec des LLMs\\n- La gestion de la m√©moire (pour garder le contexte entre les requ√™tes)\\n- L'int√©gration avec des API externes ou des bases de donn√©es\\n- La cr√©ation de flux de travail personnalis√©s\\n\\nLangChain permet donc de construire des applications qui exploitent la puissance des LLMs tout en ajoutant des fonctionnalit√©s suppl√©mentaires et en g√©rant la logique d'application.\\n\\n### En r√©sum√©\\n- **LLM** : Un mod√®le de langage puissant capable de g√©n√©rer et de comprendre du texte.\\n- **LangChain** : Une biblioth√®que qui facilite l'utilisation des LLMs dans des applications plus complexes, en ajoutant des fonctionnalit√©s et en orchestrant des interactions.\\n\\nEn gros, vous pouvez voir LangChain comme un outil qui vous aide √† tirer le meilleur parti des LLMs dans vos projets de d√©veloppement.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "parser.invoke(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Ici, on vient d‚Äôinstancier le **sch√©ma pr√©c√©dent** :\n",
    "\n",
    "```text\n",
    "variables -> PromptTemplate -> LLM -> OutputParser -> string\n",
    "```\n",
    "\n",
    "LangChain g√®re pour nous :\n",
    "- la cr√©ation des messages,\n",
    "- l‚Äôappel mod√®le,\n",
    "- la conversion de la r√©ponse en `str`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Ajouter un peu de \"contexte\" (mini-RAG sans base vectorielle)\n",
    "\n",
    "On simule ici un RAG **tr√®s simplifi√©** :  \n",
    "- un mini \"corpus\" en m√©moire,  \n",
    "- une fonction de r√©cup√©ration na√Øve (recherche par mots-cl√©s),  \n",
    "- un prompt qui combine **question + contexte**.\n",
    "\n",
    "L‚Äôobjectif est juste de montrer la **brique \"retrieval\"** dans le pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un agent d√©terministe suit un sc√©nario fixe, sans appel √† un mod√®le de langage √† chaque √©tape.\n",
      "\n",
      "---\n",
      "\n",
      "Un agent IA utilise un mod√®le de langage pour d√©cider dynamiquement des actions √† chaque √©tape.\n"
     ]
    }
   ],
   "source": [
    "# Mini \"base de connaissances\" en m√©moire\n",
    "DOCUMENTS = [\n",
    "    {\n",
    "        \"title\": \"Agents d√©terministes\",\n",
    "        \"content\": \"Un agent d√©terministe suit un sc√©nario fixe, sans appel √† un mod√®le de langage √† chaque √©tape.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Agents IA (LLM)\",\n",
    "        \"content\": \"Un agent IA utilise un mod√®le de langage pour d√©cider dynamiquement des actions √† chaque √©tape.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"LangChain\",\n",
    "        \"content\": \"LangChain est un framework qui aide √† structurer les appels aux LLM et √† int√©grer des donn√©es, des outils et de la m√©moire.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "def simple_retriever(query: str, k: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Tr√®s simple :\n",
    "    - calcule un score bas√© sur le nombre de tokens communs\n",
    "    - renvoie les k meilleurs documents concat√©n√©s\n",
    "    \"\"\"\n",
    "    tokens = set(query.lower().split())\n",
    "    scored = []\n",
    "    for doc in DOCUMENTS:\n",
    "        score = len(tokens.intersection(set(doc[\"content\"].lower().split())))\n",
    "        scored.append((score, doc))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_docs = [doc[\"content\"] for score, doc in scored[:k] if score > 0]\n",
    "    if not top_docs:\n",
    "        return \"Aucun document pertinent trouv√© dans la base locale.\"\n",
    "    return \"\\n\\n---\\n\\n\".join(top_docs)\n",
    "\n",
    "print(simple_retriever(\"Quelle diff√©rence entre agent IA et agent d√©terministe ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó Cha√Æne RAG simplifi√©e \n",
    "\n",
    "On utilise `RunnableLambda` et `RunnablePassthrough` pour construire un pipeline :\n",
    "\n",
    "```text\n",
    "question\n",
    "  ‚îú‚îÄ‚îÄ> (pass-through) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îî‚îÄ‚îÄ> retriever(question) -> contexte‚î§\n",
    "                                      v\n",
    "                           Prompt(question + contexte)\n",
    "                                    |\n",
    "                                    v\n",
    "                                 LLM\n",
    "                                    |\n",
    "                                    v\n",
    "                              OutputParser\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un agent d√©terministe suit un sc√©nario fixe et pr√©√©tabli, sans faire appel √† un mod√®le de langage √† chaque √©tape. En revanche, un agent IA, comme ceux qui utilisent des mod√®les de langage, peut adapter ses r√©ponses et ses actions en fonction des donn√©es et des interactions en temps r√©el.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# √âtape 1 : pr√©parer le \"retriever\" comme Runnable\n",
    "retriever_runnable = RunnableLambda(lambda q: simple_retriever(q))\n",
    "\n",
    "# √âtape 2 : d√©finir le prompt qui int√®gre question + contexte\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Tu es un assistant qui r√©pond en t'appuyant STRICTEMENT sur le contexte fourni.\\n\"\n",
    "            \"Si une information n'est pas dans le contexte, dis-le explicitement.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Contexte :\\n{context}\\n\\n\"\n",
    "            \"Question utilisateur : {question}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever_runnable,\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"Explique la diff√©rence entre un agent d√©terministe et un agent IA.\"\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f413011",
   "metadata": {},
   "source": [
    "## ‚úÖ R√©cap\n",
    " \n",
    "Briques pr√©sent√©es dans ce notebook :\n",
    "  - mod√®le de langage (`ChatOpenAI`),\n",
    "  - prompts (`ChatPromptTemplate`),\n",
    "  - pipeline (`chain = prompt | model | parser`),\n",
    "  - retrieval/RAG (m√™me tr√®s simplifi√©).\n",
    "\n",
    "Pour la suite nous allons aborder :  \n",
    "- l‚Äôorchestration **multi-√©tapes** avec LangGraph,  \n",
    "- puis le **pattern d‚Äôagent** plus avanc√© dans le dernier notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
