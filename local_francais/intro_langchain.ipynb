{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prendre en main LangChain pas √† pas\n",
    "\n",
    "Objectif de ce notebook :  \n",
    "- Comprendre les **briques de base** de LangChain (LLM, prompt, chain).  \n",
    "- Visualiser un **sch√©ma simple** de pipeline.  \n",
    "- Construire un mini **pipeline de question/r√©ponse**, puis un mini **pipeline RAG simplifi√©**.\n",
    "\n",
    "> Remarque : ce notebook utilise un **LLM local** (Ollama ou LM Studio) configur√© via le fichier `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les briques de base de LangChain\n",
    "\n",
    "On va manipuler 3 briques principales :\n",
    "\n",
    "1. **LLM** : le mod√®le de langage (par ex. `gpt-4o-mini`)  \n",
    "2. **Prompt** : la mani√®re dont on parle au mod√®le (system + user + variables)  \n",
    "3. **Chain** : l‚Äôassemblage **d√©claratif** des √©tapes (pipeline de traitement)\n",
    "\n",
    "On peut imaginer ce sch√©ma tr√®s simple :\n",
    "\n",
    "```text\n",
    "              +---------------------------+\n",
    "User input -> |   PromptTemplate          | -> texte format√©\n",
    "              +---------------------------+\n",
    "                           |\n",
    "                           v\n",
    "                     +-----------+\n",
    "                     |   LLM     | -> r√©ponse brute du mod√®le\n",
    "                     +-----------+\n",
    "                           |\n",
    "                           v\n",
    "                   +------------------+\n",
    "                   | Output Parser    | -> string / JSON / etc.\n",
    "                   +------------------+\n",
    "```  \n",
    "LangChain permet de **d√©crire ce pipeline en Python**, plut√¥t que de tout reprogrammer √† la main."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Installation (√† faire une fois dans votre environnement)\n",
    "\n",
    "D√©commentez et ex√©cutez la cellule ci-dessous si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (1.2.5)\n",
      "Requirement already satisfied: langchain-ollama in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core) (0.5.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.6.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-ollama) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.6.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.3.1)\n",
      "Requirement already satisfied: langchain-openai in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (1.1.6)\n",
      "Requirement already satisfied: python-dotenv in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.2 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-openai) (1.2.5)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-openai) (2.14.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.5.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.2->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.2->langchain-openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/daparicio/code/github.com/davidaparicio/azur-tech-winter-25-ai-agents-tutorial/.venv/lib/python3.10/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (2.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-core langchain-ollama python-dotenv\n",
    "# Pour LM Studio (API compatible OpenAI):\n",
    "!pip install -U langchain-openai python-dotenv\n",
    "# Optionnel : outils suppl√©mentaires\n",
    "# !pip install -U langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Initialisation du LLM\n",
    "\n",
    "On cr√©e un objet LLM local qui sera la brique \"mod√®le de langage\" c√¥t√© LangChain.\n",
    "Vous pouvez utiliser soit **Ollama** soit **LM Studio** en modifiant le fichier `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de LM Studio avec le mod√®le mistralai/devstral-small-2-2512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={}, client=<openai.resources.chat.completions.completions.Completions object at 0x11139dc60>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11139e140>, root_client=<openai.OpenAI object at 0x11139de10>, root_async_client=<openai.AsyncOpenAI object at 0x11139e020>, model_name='mistralai/devstral-small-2-2512', temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='http://localhost:1234/v1')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration du LLM local\n",
    "llm_type = os.getenv(\"LOCAL_LLM_TYPE\", \"ollama\")\n",
    "\n",
    "if llm_type == \"ollama\":\n",
    "    from langchain_ollama import ChatOllama\n",
    "    \n",
    "    ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "    ollama_model = os.getenv(\"OLLAMA_MODEL\", \"llama3.2:latest\")\n",
    "    \n",
    "    model = ChatOllama(\n",
    "        model=ollama_model,\n",
    "        base_url=ollama_base_url,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    print(f\"Utilisation d'Ollama avec le mod√®le {ollama_model}\")\n",
    "    \n",
    "elif llm_type == \"lmstudio\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    lmstudio_base_url = os.getenv(\"LMSTUDIO_BASE_URL\", \"http://localhost:1234/v1\")\n",
    "    lmstudio_model = os.getenv(\"LMSTUDIO_MODEL\", \"local-model\")\n",
    "    \n",
    "    model = ChatOpenAI(\n",
    "        model=lmstudio_model,\n",
    "        base_url=lmstudio_base_url,\n",
    "        api_key=\"not-needed\",  # LM Studio ne n√©cessite pas de cl√© API\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    print(f\"Utilisation de LM Studio avec le mod√®le {lmstudio_model}\")\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"LOCAL_LLM_TYPE non reconnu: {llm_type}. Utilisez 'ollama' ou 'lmstudio'.\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Premier appel au LLM via LangChain\n",
    "\n",
    "On commence par l‚Äôusage le plus simple : on envoie une liste de messages au mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un mod√®le de langage est un algorithme d'intelligence artificielle entra√Æn√© sur d'√©normes quantit√©s de texte pour comprendre et g√©n√©rer du langage humain. Il utilise des techniques comme le *deep learning* (r√©seaux de neurones) pour pr√©dire et produire des phrases coh√©rentes en fonction du contexte donn√©.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Tu es un assistant p√©dagogique, clair et concis.\"),\n",
    "    HumanMessage(content=\"Explique en deux phrases ce qu'est un mod√®le de langage.\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ S√©parer le **prompt** et le **mod√®le**\n",
    "\n",
    "Plut√¥t que de construire les messages √† la main, on d√©crit un **template de conversation**.\n",
    "\n",
    "On utilise `ChatPromptTemplate` pour d√©finir :\n",
    "- le r√¥le syst√®me (persona),\n",
    "- le message utilisateur avec une **variable `{question}`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Ah, excellente question ! Je vais te l\\'expliquer simplement avec des analogies et des concepts que tu connais d√©j√† en tant que d√©veloppeur.\\n\\n### **1. LLM (Large Language Model) : Le \"Cerveau\"**\\nUn LLM, c\\'est comme un mod√®le de langage pr√©-entra√Æn√© (ex: GPT-3, GPT-4, Llama) qui a appris √† comprendre et g√©n√©rer du texte en analysant des milliards de donn√©es. C\\'est un peu comme une **bo√Æte noire** qui prend du texte en entr√©e et produit du texte en sortie.\\n\\n- **Exemple concret** :\\n  Si tu lui donnes la prompt `\"√âcris un code Python pour calculer la factorielle d\\'un nombre\"`, il va te r√©pondre avec une fonction comme `def factorial(n): ...`.\\n\\n- **Limites** :\\n  - Il ne sait pas faire grand-chose *seul* : pas d\\'acc√®s √† des bases de donn√©es, pas de m√©moire entre les requ√™tes (sauf avec des techniques comme le *few-shot learning*), et pas de logique complexe sans guidage.\\n\\n---\\n\\n### **2. LangChain : Le \"Framework\" pour Super-Pouvoirs**\\nLangChain, c\\'est une **biblioth√®que** (en Python) qui te permet de **combiner un LLM avec d\\'autres outils** pour cr√©er des applications plus puissantes.\\n\\nC\\'est comme si tu avais un LLM (le cerveau) et que LangChain te fournissait :\\n- **Des \"modules\" pour √©tendre ses capacit√©s** (ex: m√©moire, acc√®s √† des APIs, outils externes).\\n- **Des \"cha√Ænes\" (chains) pour orchestrer des t√¢ches complexes**.\\n\\n#### **Exemples d\\'utilisation avec LangChain** :\\n1. **M√©moire (Memory)** :\\n   - Tu peux stocker l\\'historique des conversations pour que le LLM \"se souvienne\" de ce qui a √©t√© dit avant.\\n   - *Exemple* : Un chatbot qui se souvient que tu as demand√© la m√©t√©o hier et te propose une mise √† jour aujourd\\'hui.\\n\\n2. **Outils Externes (Tools)** :\\n   - Le LLM peut appeler des APIs, faire des requ√™tes SQL, ou m√™me ex√©cuter du code.\\n   - *Exemple* : Un assistant qui utilise Google Maps pour r√©pondre √† `\"O√π est le meilleur restaurant italien pr√®s de chez moi ?\"`.\\n\\n3. **Cha√Ænes (Chains)** :\\n   - Combinaison de plusieurs √©tapes pour r√©soudre un probl√®me.\\n   - *Exemple* :\\n     1. Le LLM g√©n√®re une question en SQL √† partir de ta requ√™te naturelle.\\n     2. LangChain ex√©cute la requ√™te sur une base de donn√©es.\\n     3. Le LLM reformate le r√©sultat en langage naturel.\\n\\n---\\n\\n### **Analogie avec un D√©veloppeur**\\n- **LLM seul** = Un d√©veloppeur qui sait coder mais n\\'a pas acc√®s √† Google, Stack Overflow ou son IDE.\\n- **LLM + LangChain** = Ce m√™me d√©veloppeur avec acc√®s √† tous ses outils pr√©f√©r√©s (GitHub, Docker, Postman...) pour r√©soudre des probl√®mes complexes.\\n\\n---\\n\\n### **En R√©sum√©**\\n| Concept       | R√¥le                                                                 |\\n|--------------|------------------------------------------------------------------------|\\n| **LLM**      | Mod√®le de langage (ex: GPT-4) qui g√©n√®re du texte √† partir d\\'une prompt. |\\n| **LangChain**| Framework pour √©tendre le LLM avec des outils, de la m√©moire et des workflows. |\\n\\n**Exemple d\\'application avec les deux** :\\n- Un bot qui r√©sume des articles en temps r√©el en cherchant des infos sur le web ‚Üí **LLM** pour la compr√©hension, **LangChain** pour l\\'acc√®s aux donn√©es et la m√©moire.\\n\\nTu veux un exemple de code pour voir comment √ßa marche en pratique ? üòä' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 809, 'prompt_tokens': 36, 'total_tokens': 845, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'mistralai/devstral-small-2-2512', 'system_fingerprint': 'mistralai/devstral-small-2-2512', 'id': 'chatcmpl-apsjkkjzsr5sf2ejl3so88', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b50d4-c532-7fd3-b308-1a120af8389f-0' usage_metadata={'input_tokens': 36, 'output_tokens': 809, 'total_tokens': 845, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", f\"Tu es un assistant IA qui vulgarise pour un public de d√©veloppeurs curieux.\"),\n",
    "        (\"human\", \"Question : {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# LCEL: LangChain Expression Language\n",
    "chain = prompt | model \n",
    "\n",
    "answer = chain.invoke({\"question\": \"Quelle est la diff√©rence entre un LLM et LangChain ?\"})\n",
    "print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a14162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ah, excellente question ! Je vais te l\\'expliquer simplement avec des analogies et des concepts que tu connais d√©j√† en tant que d√©veloppeur.\\n\\n### **1. LLM (Large Language Model) : Le \"Cerveau\"**\\nUn LLM, c\\'est comme un mod√®le de langage pr√©-entra√Æn√© (ex: GPT-3, GPT-4, Llama) qui a appris √† comprendre et g√©n√©rer du texte en analysant des milliards de donn√©es. C\\'est un peu comme une **bo√Æte noire** qui prend du texte en entr√©e et produit du texte en sortie.\\n\\n- **Exemple concret** :\\n  Si tu lui donnes la prompt `\"√âcris un code Python pour calculer la factorielle d\\'un nombre\"`, il va te r√©pondre avec une fonction comme `def factorial(n): ...`.\\n\\n- **Limites** :\\n  - Il ne sait pas faire grand-chose *seul* : pas d\\'acc√®s √† des bases de donn√©es, pas de m√©moire entre les requ√™tes (sauf avec des techniques comme le *few-shot learning*), et pas de logique complexe sans guidage.\\n\\n---\\n\\n### **2. LangChain : Le \"Framework\" pour Super-Pouvoirs**\\nLangChain, c\\'est une **biblioth√®que** (en Python) qui te permet de **combiner un LLM avec d\\'autres outils** pour cr√©er des applications plus puissantes.\\n\\nC\\'est comme si tu avais un LLM (le cerveau) et que LangChain te fournissait :\\n- **Des \"modules\" pour √©tendre ses capacit√©s** (ex: m√©moire, acc√®s √† des APIs, outils externes).\\n- **Des \"cha√Ænes\" (chains) pour orchestrer des t√¢ches complexes**.\\n\\n#### **Exemples d\\'utilisation avec LangChain** :\\n1. **M√©moire (Memory)** :\\n   - Tu peux stocker l\\'historique des conversations pour que le LLM \"se souvienne\" de ce qui a √©t√© dit avant.\\n   - *Exemple* : Un chatbot qui se souvient que tu as demand√© la m√©t√©o hier et te propose une mise √† jour aujourd\\'hui.\\n\\n2. **Outils Externes (Tools)** :\\n   - Le LLM peut appeler des APIs, faire des requ√™tes SQL, ou m√™me ex√©cuter du code.\\n   - *Exemple* : Un assistant qui utilise Google Maps pour r√©pondre √† `\"O√π est le meilleur restaurant italien pr√®s de chez moi ?\"`.\\n\\n3. **Cha√Ænes (Chains)** :\\n   - Combinaison de plusieurs √©tapes pour r√©soudre un probl√®me.\\n   - *Exemple* :\\n     1. Le LLM g√©n√®re une question en SQL √† partir de ta requ√™te naturelle.\\n     2. LangChain ex√©cute la requ√™te sur une base de donn√©es.\\n     3. Le LLM reformate le r√©sultat en langage naturel.\\n\\n---\\n\\n### **Analogie avec un D√©veloppeur**\\n- **LLM seul** = Un d√©veloppeur qui sait coder mais n\\'a pas acc√®s √† Google, Stack Overflow ou son IDE.\\n- **LLM + LangChain** = Ce m√™me d√©veloppeur avec acc√®s √† tous ses outils pr√©f√©r√©s (GitHub, Docker, Postman...) pour r√©soudre des probl√®mes complexes.\\n\\n---\\n\\n### **En R√©sum√©**\\n| Concept       | R√¥le                                                                 |\\n|--------------|------------------------------------------------------------------------|\\n| **LLM**      | Mod√®le de langage (ex: GPT-4) qui g√©n√®re du texte √† partir d\\'une prompt. |\\n| **LangChain**| Framework pour √©tendre le LLM avec des outils, de la m√©moire et des workflows. |\\n\\n**Exemple d\\'application avec les deux** :\\n- Un bot qui r√©sume des articles en temps r√©el en cherchant des infos sur le web ‚Üí **LLM** pour la compr√©hension, **LangChain** pour l\\'acc√®s aux donn√©es et la m√©moire.\\n\\nTu veux un exemple de code pour voir comment √ßa marche en pratique ? üòä'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "parser.invoke(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Ici, on vient d‚Äôinstancier le **sch√©ma pr√©c√©dent** :\n",
    "\n",
    "```text\n",
    "variables -> PromptTemplate -> LLM -> OutputParser -> string\n",
    "```\n",
    "\n",
    "LangChain g√®re pour nous :\n",
    "- la cr√©ation des messages,\n",
    "- l‚Äôappel mod√®le,\n",
    "- la conversion de la r√©ponse en `str`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Ajouter un peu de \"contexte\" (mini-RAG sans base vectorielle)\n",
    "\n",
    "On simule ici un RAG **tr√®s simplifi√©** :  \n",
    "- un mini \"corpus\" en m√©moire,  \n",
    "- une fonction de r√©cup√©ration na√Øve (recherche par mots-cl√©s),  \n",
    "- un prompt qui combine **question + contexte**.\n",
    "\n",
    "L‚Äôobjectif est juste de montrer la **brique \"retrieval\"** dans le pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un agent d√©terministe suit un sc√©nario fixe, sans appel √† un mod√®le de langage √† chaque √©tape.\n",
      "\n",
      "---\n",
      "\n",
      "Un agent IA utilise un mod√®le de langage pour d√©cider dynamiquement des actions √† chaque √©tape.\n"
     ]
    }
   ],
   "source": [
    "# Mini \"base de connaissances\" en m√©moire\n",
    "DOCUMENTS = [\n",
    "    {\n",
    "        \"title\": \"Agents d√©terministes\",\n",
    "        \"content\": \"Un agent d√©terministe suit un sc√©nario fixe, sans appel √† un mod√®le de langage √† chaque √©tape.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Agents IA (LLM)\",\n",
    "        \"content\": \"Un agent IA utilise un mod√®le de langage pour d√©cider dynamiquement des actions √† chaque √©tape.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"LangChain\",\n",
    "        \"content\": \"LangChain est un framework qui aide √† structurer les appels aux LLM et √† int√©grer des donn√©es, des outils et de la m√©moire.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "def simple_retriever(query: str, k: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Tr√®s simple :\n",
    "    - calcule un score bas√© sur le nombre de tokens communs\n",
    "    - renvoie les k meilleurs documents concat√©n√©s\n",
    "    \"\"\"\n",
    "    tokens = set(query.lower().split())\n",
    "    scored = []\n",
    "    for doc in DOCUMENTS:\n",
    "        score = len(tokens.intersection(set(doc[\"content\"].lower().split())))\n",
    "        scored.append((score, doc))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_docs = [doc[\"content\"] for score, doc in scored[:k] if score > 0]\n",
    "    if not top_docs:\n",
    "        return \"Aucun document pertinent trouv√© dans la base locale.\"\n",
    "    return \"\\n\\n---\\n\\n\".join(top_docs)\n",
    "\n",
    "print(simple_retriever(\"Quelle diff√©rence entre agent IA et agent d√©terministe ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó Cha√Æne RAG simplifi√©e \n",
    "\n",
    "On utilise `RunnableLambda` et `RunnablePassthrough` pour construire un pipeline :\n",
    "\n",
    "```text\n",
    "question\n",
    "  ‚îú‚îÄ‚îÄ> (pass-through) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îî‚îÄ‚îÄ> retriever(question) -> contexte‚î§\n",
    "                                      v\n",
    "                           Prompt(question + contexte)\n",
    "                                    |\n",
    "                                    v\n",
    "                                 LLM\n",
    "                                    |\n",
    "                                    v\n",
    "                              OutputParser\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans le contexte fourni, un **agent d√©terministe** suit un sc√©nario fixe sans appel √† un mod√®le de langage (LLM) √† chaque √©tape. Cela signifie que ses actions sont pr√©programm√©es et ne d√©pendent pas d'une intelligence artificielle pour prendre des d√©cisions en temps r√©el.\n",
      "\n",
      "En revanche, un **agent IA** (ou agent bas√© sur LLM) utilise un mod√®le de langage pour g√©n√©rer des r√©ponses ou des actions dynamiques en fonction du contexte, des donn√©es d'entr√©e et √©ventuellement de la m√©moire. LangChain, par exemple, est un framework qui facilite l'int√©gration de ces agents IA en structurant les appels aux LLM et en ajoutant des outils ou des donn√©es pour enrichir leurs capacit√©s.\n",
      "\n",
      "Si vous souhaitez une comparaison plus d√©taill√©e, il faudrait pr√©ciser le contexte des agents IA (par exemple, leur architecture ou leur utilisation dans LangChain).\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# √âtape 1 : pr√©parer le \"retriever\" comme Runnable\n",
    "retriever_runnable = RunnableLambda(lambda q: simple_retriever(q))\n",
    "\n",
    "# √âtape 2 : d√©finir le prompt qui int√®gre question + contexte\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Tu es un assistant qui r√©pond en t'appuyant STRICTEMENT sur le contexte fourni.\\n\"\n",
    "            \"Si une information n'est pas dans le contexte, dis-le explicitement.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Contexte :\\n{context}\\n\\n\"\n",
    "            \"Question utilisateur : {question}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever_runnable,\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"Explique la diff√©rence entre un agent d√©terministe et un agent IA.\"\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f413011",
   "metadata": {},
   "source": [
    "## ‚úÖ R√©cap\n",
    " \n",
    "Briques pr√©sent√©es dans ce notebook :\n",
    "  - mod√®le de langage (`ChatOpenAI`),\n",
    "  - prompts (`ChatPromptTemplate`),\n",
    "  - pipeline (`chain = prompt | model | parser`),\n",
    "  - retrieval/RAG (m√™me tr√®s simplifi√©).\n",
    "\n",
    "Pour la suite nous allons aborder :  \n",
    "- l‚Äôorchestration **multi-√©tapes** avec LangGraph,  \n",
    "- puis le **pattern d‚Äôagent** plus avanc√© dans le dernier notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
