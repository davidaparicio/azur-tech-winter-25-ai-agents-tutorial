# Configuration pour LLM local (Ollama ou LM Studio)

# Type de LLM local: "ollama" ou "lmstudio"
LOCAL_LLM_TYPE=ollama

# Pour Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest

# Pour LM Studio (utilise une API compatible OpenAI)
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=local-model

# Optionnel: Configuration Langfuse pour l'observabilité locale
# Pour Langfuse local (self-hosted)
# LANGFUSE_HOST=http://localhost:3000
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_SECRET_KEY=sk-lf-...

# Pour Langfuse cloud (si vous préférez)
# LANGFUSE_HOST=https://cloud.langfuse.com
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_SECRET_KEY=sk-lf-...
