# Configuration pour LLM local (Ollama ou LM Studio)

# Type de LLM local: "ollama" ou "lmstudio"
LOCAL_LLM_TYPE=fake-type

# Pour Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=fake-model-olm

# Pour LM Studio (utilise une API compatible OpenAI)
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=fake-model-lms

# Optionnel: Configuration Langfuse pour l'observabilité locale
# Pour Langfuse local (self-hosted)
# LANGFUSE_BASE_URL=http://localhost:3003
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_SECRET_KEY=sk-lf-...

# Pour Langfuse cloud (si vous préférez)
# LANGFUSE_BASE_URL=https://cloud.langfuse.com
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_SECRET_KEY=sk-lf-...
