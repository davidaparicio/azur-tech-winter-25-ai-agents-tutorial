# Tutoriel sur les Agents IA - Telecom Valley Winter Tech 2025

Ce d√©p√¥t contient un tutoriel sur la construction d'agents IA en utilisant LangChain et LangGraph, pr√©sent√© lors de Telecom Valley Winter Tech 2025.

## üìö Contenu

Ce tutoriel se compose de trois notebooks progressifs :

1. **`intro_langchain.ipynb`** - Introduction aux fondamentaux de LangChain

   - Comprendre les briques de base (LLM, prompts, cha√Ænes)
   - Construire des pipelines simples de question/r√©ponse
   - Cr√©er un syst√®me RAG (Retrieval-Augmented Generation) simplifi√©

2. **`intro_langgraph.ipynb`** - Introduction √† LangGraph

   - Comprendre les concepts de StateGraph
   - Construire des workflows sous forme de graphes
   - Cr√©er un agent conversationnel simple

3. **`agents_meteo_calendrier.ipynb`** - Agents ReAct avec LangGraph
   - Construire un agent ReAct utilisant deux outils : m√©t√©o et calendrier
   - Impl√©menter le m√™me comportement avec LangGraph (workflow explicite)
   - Comparer les approches ReAct vs LangGraph
   - Observabilit√© avec Langfuse (local)

## üöÄ Configuration

### Pr√©requis

- Python 3.8+
- Jupyter Notebook ou JupyterLab

### Installation

1. Clonez ce d√©p√¥t :

```bash
git clone <repository-url>
cd Telecom-Valley-Winter-Tech-2025/ai-agents-tutorial/francais
```

2. Installez les packages requis :

```bash
# Pour Ollama (recommand√©)
pip install -U langchain-core langchain-ollama langgraph langchain-text-splitters python-dotenv

# OU pour LM Studio (API compatible OpenAI)
pip install -U langchain-core langchain-openai langgraph langchain-text-splitters python-dotenv

# Pour l'observabilit√© avec Langfuse (optionnel)
pip install -U langfuse
```

Pour les fonctionnalit√©s de visualisation (g√©n√©ration PNG Mermaid) :

```bash
pip install -U grandalf
```

### Configuration du LLM local

Ce tutoriel utilise un **LLM local** au lieu de l'API OpenAI. Vous avez le choix entre **Ollama** ou **LM Studio**.

#### Option 1 : Ollama (Recommand√©)

1. Installez Ollama depuis [ollama.ai](https://ollama.ai)

2. T√©l√©chargez un mod√®le (par exemple llama3.2) :

```bash
ollama pull llama3.2:latest
```

3. V√©rifiez que le serveur Ollama fonctionne :

```bash
ollama serve
```

#### Option 2 : LM Studio

1. Installez LM Studio depuis [lmstudio.ai](https://lmstudio.ai)

2. T√©l√©chargez un mod√®le via l'interface LM Studio

3. D√©marrez le serveur local (par d√©faut sur http://localhost:1234)

### Variables d'environnement

Cr√©ez un fichier `.env` dans le r√©pertoire `francais` (un fichier `.env.example` est fourni comme mod√®le) :

```bash
# Type de LLM local: "ollama" ou "lmstudio"
LOCAL_LLM_TYPE=ollama

# Configuration Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest

# Configuration LM Studio (si vous utilisez LM Studio)
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=local-model

# Optionnel : Langfuse pour l'observabilit√© locale
# LANGFUSE_BASE_URL=http://localhost:3000
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_SECRET_KEY=sk-lf-...
```

**Important** : Le fichier `.env` est ignor√© par git pour √©viter de committer des informations sensibles. Utilisez `.env.example` comme mod√®le.

### Configuration de Langfuse (Optionnel)

Langfuse est une plateforme d'observabilit√© open-source pour les applications LLM. Contrairement √† LangSmith, elle peut √™tre h√©berg√©e localement.

#### Option 1 : Langfuse local avec Docker (Recommand√©)

1. Installez Docker sur votre machine

2. Lancez Langfuse avec Docker Compose (un fichier `docker-compose.yml` est d√©j√† fourni dans ce r√©pertoire) :

```bash
# Lancez Langfuse
docker-compose up -d
```

3. Acc√©dez √† l'interface web : http://localhost:3000

4. Cr√©ez un compte et un projet

5. R√©cup√©rez vos cl√©s API (Public Key et Secret Key) dans les param√®tres du projet

6. Installez le package Python :

```bash
pip install langfuse
```

7. Ajoutez les cl√©s dans votre fichier `.env`

#### Option 2 : Langfuse Cloud

Si vous pr√©f√©rez ne pas h√©berger localement :

1. Cr√©ez un compte sur [cloud.langfuse.com](https://cloud.langfuse.com)

2. Cr√©ez un projet et r√©cup√©rez vos cl√©s API

3. Installez le package Python :

```bash
pip install langfuse
```

4. Dans votre `.env`, utilisez :

```bash
LANGFUSE_BASE_URL=https://cloud.langfuse.com
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
```

## üìñ Utilisation

1. D√©marrez Jupyter Notebook :

```bash
jupyter notebook
```

2. Ouvrez les notebooks dans l'ordre :

   - Commencez par `intro_langchain.ipynb` pour les bases de LangChain
   - Puis `intro_langgraph.ipynb` pour les concepts LangGraph
   - Enfin `agents_meteo_calendrier.ipynb` pour l'exemple complet d'agent

3. Assurez-vous que vos variables d'environnement sont d√©finies avant d'ex√©cuter les cellules qui n√©cessitent des cl√©s API.

## üìÑ Diapositives

**Note** : Le fichier `slides.pdf` est inclus pour des raisons de commodit√© de partage.

## üõ†Ô∏è Technologies utilis√©es

- **LangChain** : Framework pour construire des applications avec des LLM
- **LangGraph** : Biblioth√®que pour construire des applications multi-acteurs avec √©tat en utilisant des LLM
- **Ollama / LM Studio** : Solutions de LLM locaux pour ex√©cuter des mod√®les de langage sans d√©pendre d'API externes
- **Langfuse** : Plateforme d'observabilit√© open-source pour les applications LLM, h√©bergeable localement (optionnel)

## üìù Notes

- Les outils m√©t√©o et calendrier dans `agents_meteo_calendrier.ipynb` sont simul√©s (pas d'appels API externes)
- Tous les notebooks incluent des commentaires et explications en fran√ßais
- Le tutoriel d√©montre √† la fois les workflows d'agents implicites (ReAct) et explicites (LangGraph)

## ü§ù Contribution

N'h√©sitez pas √† ouvrir des issues ou √† soumettre des pull requests pour des am√©liorations.

## üë§ Auteur

Pr√©sent√© par Marie Vaucher lors de Telecom Valley Winter Tech 2025
marie@aztelia.com
+33 7 83 17 88 35
