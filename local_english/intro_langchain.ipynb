{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Getting Started with LangChain Step by Step\n\nObjective of this notebook:  \n- Understand the **basic building blocks** of LangChain (LLM, prompt, chain).  \n- Visualize a **simple pipeline** schema.  \n- Build a mini **question/answer pipeline**, then a mini **simplified RAG pipeline**.\n\n> Note: this notebook uses a **local LLM** (Ollama or LM Studio) configured via the `.env` file."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Building Blocks of LangChain\n",
    "\n",
    "We will work with 3 main building blocks:\n",
    "\n",
    "1. **LLM**: the language model (e.g., `gpt-4o-mini`)  \n",
    "2. **Prompt**: how we talk to the model (system + user + variables)  \n",
    "3. **Chain**: the **declarative** assembly of steps (processing pipeline)\n",
    "\n",
    "We can imagine this very simple schema:\n",
    "\n",
    "```text\n",
    "              +---------------------------+\n",
    "User input -> |   PromptTemplate          | -> formatted text\n",
    "              +---------------------------+\n",
    "                           |\n",
    "                           v\n",
    "                     +-----------+\n",
    "                     |   LLM     | -> raw model response\n",
    "                     +-----------+\n",
    "                           |\n",
    "                           v\n",
    "                   +------------------+\n",
    "                   | Output Parser    | -> string / JSON / etc.\n",
    "                   +------------------+\n",
    "```  \n",
    "LangChain allows us to **describe this pipeline in Python**, rather than reprogramming everything manually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Installation (do once in your environment)\n",
    "\n",
    "Uncomment and execute the cell below if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# !pip install -U langchain-core langchain-ollama python-dotenv\n# For LM Studio (OpenAI-compatible API):\n# !pip install -U langchain-openai python-dotenv\n# Optional: additional tools\n# !pip install -U langchain-text-splitters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## âš™ï¸ LLM Initialization\n\nWe create a local LLM object which is the \"language model\" building block in LangChain.\nYou can use either **Ollama** or **LM Studio** by modifying the `.env` file."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Local LLM configuration\nllm_type = os.getenv(\"LOCAL_LLM_TYPE\", \"ollama\")\n\nif llm_type == \"ollama\":\n    from langchain_ollama import ChatOllama\n    \n    ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n    ollama_model = os.getenv(\"OLLAMA_MODEL\", \"llama3.2:latest\")\n    \n    model = ChatOllama(\n        model=ollama_model,\n        base_url=ollama_base_url,\n        temperature=0.2,\n    )\n    print(f\"Using Ollama with model {ollama_model}\")\n    \nelif llm_type == \"lmstudio\":\n    from langchain_openai import ChatOpenAI\n    \n    lmstudio_base_url = os.getenv(\"LMSTUDIO_BASE_URL\", \"http://localhost:1234/v1\")\n    lmstudio_model = os.getenv(\"LMSTUDIO_MODEL\", \"local-model\")\n    \n    model = ChatOpenAI(\n        model=lmstudio_model,\n        base_url=lmstudio_base_url,\n        api_key=\"not-needed\",  # LM Studio doesn't require an API key\n        temperature=0.2,\n    )\n    print(f\"Using LM Studio with model {lmstudio_model}\")\n    \nelse:\n    raise ValueError(f\"Unrecognized LOCAL_LLM_TYPE: {llm_type}. Use 'ollama' or 'lmstudio'.\")\n\nmodel"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ First LLM Call via LangChain\n",
    "\n",
    "We start with the simplest usage: we send a list of messages to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an educational assistant, clear and concise.\"),\n",
    "    HumanMessage(content=\"Explain in two sentences what a language model is.\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Separate the **prompt** and the **model**\n",
    "\n",
    "Rather than building messages manually, we describe a **conversation template**.\n",
    "\n",
    "We use `ChatPromptTemplate` to define:\n",
    "- the system role (persona),\n",
    "- the user message with a **variable `{question}`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", f\"You are an AI assistant who explains things for an audience of curious developers.\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# LCEL: LangChain Expression Language\n",
    "chain = prompt | model \n",
    "\n",
    "answer = chain.invoke({\"question\": \"What is the difference between an LLM and LangChain?\"})\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "parser.invoke(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ Here, we just instantiated the **previous schema**:\n",
    "\n",
    "```text\n",
    "variables -> PromptTemplate -> LLM -> OutputParser -> string\n",
    "```\n",
    "\n",
    "LangChain handles for us:\n",
    "- message creation,\n",
    "- model call,\n",
    "- response conversion to `str`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Add some \"context\" (mini-RAG without vector database)\n",
    "\n",
    "We simulate here a **very simplified** RAG:  \n",
    "- a mini \"corpus\" in memory,  \n",
    "- a naive retrieval function (keyword search),  \n",
    "- a prompt that combines **question + context**.\n",
    "\n",
    "The goal is just to show the **\"retrieval\" building block** in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini \"knowledge base\" in memory\n",
    "DOCUMENTS = [\n",
    "    {\n",
    "        \"title\": \"Deterministic Agents\",\n",
    "        \"content\": \"A deterministic agent follows a fixed scenario, without calling a language model at each step.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"AI Agents (LLM)\",\n",
    "        \"content\": \"An AI agent uses a language model to dynamically decide actions at each step.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"LangChain\",\n",
    "        \"content\": \"LangChain is a framework that helps structure LLM calls and integrate data, tools, and memory.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "def simple_retriever(query: str, k: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Very simple:\n",
    "    - calculates a score based on the number of common tokens\n",
    "    - returns the k best documents concatenated\n",
    "    \"\"\"\n",
    "    tokens = set(query.lower().split())\n",
    "    scored = []\n",
    "    for doc in DOCUMENTS:\n",
    "        score = len(tokens.intersection(set(doc[\"content\"].lower().split())))\n",
    "        scored.append((score, doc))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_docs = [doc[\"content\"] for score, doc in scored[:k] if score > 0]\n",
    "    if not top_docs:\n",
    "        return \"No relevant document found in the local database.\"\n",
    "    return \"\\n\\n---\\n\\n\".join(top_docs)\n",
    "\n",
    "print(simple_retriever(\"What is the difference between an AI agent and a deterministic agent?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”— Simplified RAG Chain \n",
    "\n",
    "We use `RunnableLambda` and `RunnablePassthrough` to build a pipeline:\n",
    "\n",
    "```text\n",
    "question\n",
    "  â”œâ”€â”€> (pass-through) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â””â”€â”€> retriever(question) -> contextâ”¤\n",
    "                                      v\n",
    "                           Prompt(question + context)\n",
    "                                    |\n",
    "                                    v\n",
    "                                 LLM\n",
    "                                    |\n",
    "                                    v\n",
    "                              OutputParser\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# Step 1: prepare the \"retriever\" as Runnable\n",
    "retriever_runnable = RunnableLambda(lambda q: simple_retriever(q))\n",
    "\n",
    "# Step 2: define the prompt that integrates question + context\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an assistant who answers by STRICTLY relying on the provided context.\\n\"\n",
    "            \"If information is not in the context, say so explicitly.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Context:\\n{context}\\n\\n\"\n",
    "            \"User question: {question}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever_runnable,\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"Explain the difference between a deterministic agent and an AI agent.\"\n",
    "print(rag_chain.invoke(question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Recap\n",
    " \n",
    "Building blocks presented in this notebook:\n",
    "  - language model (`ChatOpenAI`),\n",
    "  - prompts (`ChatPromptTemplate`),\n",
    "  - pipeline (`chain = prompt | model | parser`),\n",
    "  - retrieval/RAG (even very simplified).\n",
    "\n",
    "Next we will cover:  \n",
    "- **multi-step** orchestration with LangGraph,  \n",
    "- then the more advanced **agent pattern** in the last notebook.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}