# Configuration for local LLM (Ollama or LM Studio)

# Type of local LLM: "ollama" or "lmstudio"
LOCAL_LLM_TYPE=ollama

# For Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest

# For LM Studio (uses OpenAI-compatible API)
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=local-model

# Optional: Langfuse configuration for local observability
# For local Langfuse (self-hosted)
# LANGFUSE_HOST=http://localhost:3000
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_SECRET_KEY=sk-lf-...

# For Langfuse cloud (if you prefer)
# LANGFUSE_HOST=https://cloud.langfuse.com
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_SECRET_KEY=sk-lf-...
